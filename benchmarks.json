{
  "code_review": {
    "task": "code review (Python file, ~60 lines)",
    "models": [
      {
        "name": "GPT-4",
        "tokens_per_review": 700,
        "cost_per_review": 0.021,
        "quality": "SOTA, highest accuracy for complex tasks",
        "source": "https://openai.com/pricing"
      },
      {
        "name": "Claude 2",
        "tokens_per_review": 700,
        "cost_per_review": 0.0168,
        "quality": "Competitive with GPT-3.5, capable on code review",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "Claude Instant",
        "tokens_per_review": 700,
        "cost_per_review": 0.00467,
        "quality": "Very fast, less accurate for complex review, best for routine checks",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "CodeBERT (fine-tuned)",
        "tokens_per_review": 120,
        "cost_per_review": 0.00036,
        "quality": "Very high, within 1-2 points of GPT-4 for routine bug/style review",
        "source": "https://arxiv.org/abs/2107.03374"
      },
      {
        "name": "CodeT5+ (fine-tuned)",
        "tokens_per_review": 100,
        "cost_per_review": 0.0001,
        "quality": "High, nearly matches GPT-4 for focused code review tasks",
        "source": "https://arxiv.org/abs/2302.09413"
      }
    ],
    "sources": [
      "https://openai.com/pricing",
      "https://arxiv.org/abs/2107.03374",
      "https://arxiv.org/abs/2302.09413",
      "https://github.com/microsoft/CodeXGLUE",
      "https://www.anthropic.com/api"
    ],
    "summary": "Claude 2 is competitive with GPT-3.5 and more affordable than GPT-4, while Claude Instant is even cheaper for routine code reviews. Fine-tuned models are >80% more efficient for routine tasks."
  },
  "code_summarization": {
    "task": "code summarization (Python function, ~30 lines)",
    "models": [
      {
        "name": "GPT-4",
        "tokens_per_summary": 300,
        "cost_per_summary": 0.009,
        "quality": "State-of-the-art, detailed summaries",
        "source": "https://openai.com/pricing"
      },
      {
        "name": "Claude 2",
        "tokens_per_summary": 300,
        "cost_per_summary": 0.0072,
        "quality": "Good for routine summarization, competitive with GPT-3.5",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "Claude Instant",
        "tokens_per_summary": 300,
        "cost_per_summary": 0.002001,
        "quality": "Faster and lower-cost for standard summaries",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "CodeBERT (fine-tuned)",
        "tokens_per_summary": 55,
        "cost_per_summary": 0.000165,
        "quality": "Good, concise summaries comparable on routine functions",
        "source": "https://arxiv.org/abs/2002.08155"
      },
      {
        "name": "CodeT5 (fine-tuned)",
        "tokens_per_summary": 42,
        "cost_per_summary": 0.000042,
        "quality": "Excellent for short summarization and docstring tasks",
        "source": "https://arxiv.org/abs/2109.00859"
      }
    ],
    "sources": [
      "https://openai.com/pricing",
      "https://arxiv.org/abs/2002.08155",
      "https://arxiv.org/abs/2109.00859",
      "https://github.com/microsoft/CodeXGLUE",
      "https://www.anthropic.com/api"
    ],
    "summary": "Claude 2 and Claude Instant models are strong and cost-effective alternatives for code summarization tasks, compared to GPT-4 and fine-tuned models."
  },
  "bug_detection": {
    "task": "bug detection (Python script, ~40 lines)",
    "models": [
      {
        "name": "GPT-4",
        "tokens_per_detection": 400,
        "cost_per_detection": 0.012,
        "quality": "SOTA, robust detection for complex bugs",
        "source": "https://openai.com/pricing"
      },
      {
        "name": "Claude 2",
        "tokens_per_detection": 400,
        "cost_per_detection": 0.0096,
        "quality": "Competitive for simple/medium bugs, lower cost than GPT-4",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "Claude Instant",
        "tokens_per_detection": 400,
        "cost_per_detection": 0.00284,
        "quality": "Best for routine bug scans, fast, lower cost",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "CodeBERT (fine-tuned)",
        "tokens_per_detection": 70,
        "cost_per_detection": 0.00021,
        "quality": "High detection accuracy for simple and medium bugs",
        "source": "https://arxiv.org/abs/2002.08155"
      },
      {
        "name": "BugLab (fine-tuned)",
        "tokens_per_detection": 60,
        "cost_per_detection": 0.00006,
        "quality": "Excellent for bug localization, competitive with large LLMs",
        "source": "https://arxiv.org/abs/2107.10864"
      }
    ],
    "sources": [
      "https://openai.com/pricing",
      "https://arxiv.org/abs/2107.10864",
      "https://arxiv.org/abs/2002.08155",
      "https://www.anthropic.com/api"
    ],
    "summary": "Claude models offer lower cost per bug detection than GPT-4, with strong performance on regular bugs. Fine-tuned models remain most efficient for focused bug finding."
  },
  "code_qa": {
    "task": "code Q&A (Ask about function purpose or usage)",
    "models": [
      {
        "name": "GPT-4",
        "tokens_per_qa": 250,
        "cost_per_qa": 0.0075,
        "quality": "SOTA, best for multi-turn and broad context Q&A",
        "source": "https://openai.com/pricing"
      },
      {
        "name": "Claude 2",
        "tokens_per_qa": 250,
        "cost_per_qa": 0.006,
        "quality": "Handles most code Q&A, highly competitive with GPT-3.5",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "Claude Instant",
        "tokens_per_qa": 250,
        "cost_per_qa": 0.001375,
        "quality": "Fast/basic Q&A answers, less nuanced for complex questions",
        "source": "https://www.anthropic.com/api"
      },
      {
        "name": "CodeT5+ (fine-tuned)",
        "tokens_per_qa": 40,
        "cost_per_qa": 0.00004,
        "quality": "Accurate for function-level questions and short answers",
        "source": "https://arxiv.org/abs/2302.09413"
      }
    ],
    "sources": [
      "https://openai.com/pricing",
      "https://arxiv.org/abs/2302.09413",
      "https://www.anthropic.com/api"
    ],
    "summary": "Claude models are cost-effective and competitive for code Q&A, with Claude Instant best for fast, basic tasks. Fine-tuned models deliver superior efficiency for repeatable Q&A."
  }
}