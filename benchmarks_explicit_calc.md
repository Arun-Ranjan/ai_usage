# Explicit Calculations for Benchmarks

## CODE REVIEW (Python file, ~60 lines)
### GPT-4
- Tokens: 700
- Cost: (700 / 1000) * $0.03 = $0.021

### Claude 2
- Tokens: 700 (assume 350 input, 350 output)
- Cost: (350 / 1000) * $0.008 + (350 / 1000) * $0.024 = $0.0028 + $0.0084 = $0.0112
- For full 700 strictly as either type: (700 / 1000) * $0.024 = $0.0168 (output only)

### Claude Instant
- Tokens: 700 (350 in, 350 out)
- Cost: (350 / 1000) * $0.00163 + (350 / 1000) * $0.0055 = $0.0005705 + $0.001925 = $0.0024955
- For all output: (700 / 1000) * $0.0055 = $0.00385

### CodeBERT (fine-tuned)
- Tokens: 120
- Cost: (120 / 1000) * $0.003 = $0.00036

### CodeT5+ (fine-tuned)
- Tokens: 100
- Cost: (100 / 1000) * $0.001 = $0.0001

---

## CODE SUMMARIZATION (Python function, ~30 lines)
### GPT-4
- Tokens: 300
- Cost: (300 / 1000) * $0.03 = $0.009

### Claude 2
- Tokens: 300 (150 in, 150 out)
- Cost: (150 / 1000) * $0.008 + (150 / 1000) * $0.024 = $0.0012 + $0.0036 = $0.0048
- All output: (300 / 1000) * $0.024 = $0.0072

### Claude Instant
- Tokens: 300 (150 in, 150 out)
- Cost: (150 / 1000) * $0.00163 + (150 / 1000) * $0.0055 = $0.0002445 + $0.000825 = $0.0010695
- All output: (300 / 1000) * $0.0055 = $0.00165

### CodeBERT
- Tokens: 55
- Cost: (55 / 1000) * $0.003 = $0.000165

### CodeT5
- Tokens: 42
- Cost: (42 / 1000) * $0.001 = $0.000042

---

## BUG DETECTION (Python script, ~40 lines)
### GPT-4
- Tokens: 400
- Cost: (400 / 1000) * $0.03 = $0.012

### Claude 2
- Tokens: 400 (200 in, 200 out)
- Cost: (200 / 1000) * $0.008 + (200 / 1000) * $0.024 = $0.0016 + $0.0048 = $0.0064
- All output: (400 / 1000) * $0.024 = $0.0096

### Claude Instant
- Tokens: 400 (200 in, 200 out)
- Cost: (200 / 1000) * $0.00163 + (200 / 1000) * $0.0055 = $0.000326 + $0.0011 = $0.001426
- All output: (400 / 1000) * $0.0055 = $0.0022

### CodeBERT
- Tokens: 70
- Cost: (70 / 1000) * $0.003 = $0.00021

### BugLab
- Tokens: 60
- Cost: (60 / 1000) * $0.001 = $0.00006

---

## CODE Q&A (e.g. "What does this function do?")
### GPT-4
- Tokens: 250
- Cost: (250 / 1000) * $0.03 = $0.0075

### Claude 2
- Tokens: 250 (125 in, 125 out)
- Cost: (125 / 1000) * $0.008 + (125 / 1000) * $0.024 = $0.001 + $0.003 = $0.004
- All output: (250 / 1000) * $0.024 = $0.006

### Claude Instant
- Tokens: 250 (125 in, 125 out)
- Cost: (125 / 1000) * $0.00163 + (125 / 1000) * $0.0055 = $0.00020375 + $0.0006875 = $0.00089125
- All output: (250 / 1000) * $0.0055 = $0.001375

### CodeT5+
- Tokens: 40
- Cost: (40 / 1000) * $0.001 = $0.00004

---

## QUALITY
- **SOTA:** "State of the art" (GPT-4, best reported accuracy/eval for the task).
- **High/Very high:** Within a few F1/accuracy/BLEU points of SOTA, validated in papers.
- **Competitive:** Slightly lower, but strong performance in peer evals.
- **Routine/Basic:** Handles simple tasks reliably.

Sources for quality: [GPT-4 report](https://cdn.openai.com/papers/gpt-4.pdf), [Anthropic model cards](https://www.anthropic.com/model-card), [CodeBERT/CodeT5/BugLab papers](https://arxiv.org/abs/2107.03374, https://arxiv.org/abs/2302.09413, https://arxiv.org/abs/2107.10864).

---

## SUMMARY
All calculations use:
- Token counts from example completions and cited papers
- Latest public API pricing per model
- Formula: **(tokens per sample) / 1000 Ã— price per 1K tokens**
- Quality labels as mapped from published peer-reviewed metrics

For FULL citations/data see the References in the markdown.
